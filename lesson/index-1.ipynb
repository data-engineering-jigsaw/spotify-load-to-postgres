{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b975534-11c6-4b51-9593-2d5475e21afc",
   "metadata": {},
   "source": [
    "# Workers and Queues Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b121b2a-d523-44ad-92de-1e02cc7ece8e",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f21db-2df8-4fa8-8ee4-bc1a5641e64a",
   "metadata": {},
   "source": [
    "In this lesson, we'll work with prefect deployments to schedule requests to the Spotify API.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64a7b4-ed1a-443d-9744-46d1131ef458",
   "metadata": {},
   "source": [
    "### Getting set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59145317-050a-4097-99f4-ea64b82dd43c",
   "metadata": {},
   "source": [
    "You'll notice that we are again working with the spotify codebase from the previous lab. \n",
    "\n",
    "* Do **not** create a python environment for this lab -- it will likely mess things up.  In fact, make sure there is no python environment activated\n",
    "```bash\n",
    "    deactivate\n",
    "    conda deactivate\n",
    "```\n",
    "\n",
    "* Install the libraries in the `requirements.txt` file.\n",
    "\n",
    "* Add in your spotify API keys to the `etl/spotify_extractor/.env` file.  If you do not have spotify api keys, you can see the docs on how to create them [here](https://developer.spotify.com/documentation/web-api/tutorials/getting-started). \n",
    "\n",
    "* From there run `python3 spotify_workflow.py`, which will run the workflow.  Confirm that the track data is saved in the `data` folder.  \n",
    "\n",
    "> Please look at the file itself, which should look like the following:\n",
    "```csv\n",
    ",track_id,ranking,date,playlist_id\n",
    "0,4xhsWYTOGcal8zt0J161CU,1,2024-01-11,37i9dQZEVXbLRQDuF5jeBp\n",
    "1,0mflMxspEfB0VbI1kyLiAv,2,2024-01-11,37i9dQZEVXbLRQDuF5jeBp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ffc63-1d89-45d1-be45-faabf4f2ea36",
   "metadata": {},
   "source": [
    "### Loading to a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ae355-10f6-4fab-81a3-d0e7069048e1",
   "metadata": {},
   "source": [
    "* Create the database\n",
    "\n",
    "    * Create a database in your local postgres instance called `spotify` top songs\n",
    "    * Then run the file in the migrations folder to create a new table in the database\n",
    "    \n",
    "* Connect to the database\n",
    "    * If you look in the db.py file, you can see that we have added some that uses the sqlalchemy library to connect to the database.\n",
    "\n",
    "* From there, add the following function to the `listings_adapter.py` file\n",
    "```python\n",
    "    def load_to_postgres(df, engine, table_name = 'tracks'):\n",
    "        df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "```\n",
    "The function, takes a dataframe, and uses the `to_sql` method to save it to the specified table (tracks).\n",
    "\n",
    "Make sure that the function works properly by running the corresponding code in the console, and then connecting to postgres to confirm that the data has been written.\n",
    "\n",
    "* You can also see the `read_sql` function\n",
    "    * This allows us to use pandas to read data from our postgres database\n",
    "```python\n",
    "    from sqlalchemy import text\n",
    "    def read_sql(query, engine):\n",
    "        df = pd.DataFrame(engine.connect().execute(text(query)))\n",
    "        return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b96ce-ca1f-4502-aba3-a7f7b109f575",
   "metadata": {},
   "source": [
    "```sql\n",
    "spotify=# select * from tracks limit 3;\n",
    "        track_id        | ranking |    date    |      playlist_id\n",
    "------------------------+---------+------------+------------------------\n",
    " 7gaA3wERFkFkgivjwbSvkG |       1 | 2024-01-17 | 37i9dQZEVXbLRQDuF5jeBp\n",
    " 52eIcoLUM25zbQupAZYoFh |       2 | 2024-01-17 | 37i9dQZEVXbLRQDuF5jeBp\n",
    " 4xhsWYTOGcal8zt0J161CU |       3 | 2024-01-17 | 37i9dQZEVXbLRQDuF5jeBp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f148324-36fb-4620-bbc8-c470c64efc7c",
   "metadata": {},
   "source": [
    "### Revisiting our process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea570c-3c89-44b8-9a18-12605c0cd7c4",
   "metadata": {},
   "source": [
    "So now we have code to pull from the Spotify API, and coerce the data and then save it to a csv, from there we can read the data from the csv into a dataframe, and from there into the postgres database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81b1ee-c239-4ebf-82fe-e506c65701f1",
   "metadata": {},
   "source": [
    "Now, eventually we'll schedule this to run every day.  And this makes sense as the playlist is for the [top fifty songs](https://open.spotify.com/playlist/37i9dQZEVXbLRQDuF5jeBp) in the USA. Which will change each day.\n",
    "\n",
    "But we do not just want to just keep appending to the same CSV file.  So instead let's update the write CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a8fea-a789-4416-9ac0-bbb522d50a68",
   "metadata": {},
   "source": [
    "* Update `write_to_csv`\n",
    "    * Now instead of writing to a file `./data/track_listings.csv`, update the `write_to_csv` function so that the date is included in the file name.  For example, if the date is Jan 17 2024 the track listings file path should be `./data/2024-01-17-track_listings.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e092c42-e975-458e-801c-8aeae8e4558d",
   "metadata": {},
   "source": [
    "* `find_recent_files`\n",
    "    * So now we can `write_to_csv`, and then load the csv files into the database.  However, we want to make sure we are not re-inserting the same data into the database.  To prevent against this, write a function that only finds files later than the most recently added data:\n",
    "        * First use the `read_sql` function to find for the most recent date that we loaded data into the database.\n",
    "        * Then only return the file names after that date.\n",
    "        * For example, you can see that we have included files from 2024, and 2025.  The function should return a list of files with the file from 2025. \n",
    "\n",
    "* `load_files_to_postgres`\n",
    "    * The `load_files_to_postgres` calls the `find_recent_files` function, and then reads those csv files, and loads them to a database. \n",
    "    * You can see in the `2025` csv file that we added a track `sample-data` that should be loaded into postgres  after calling the `load_files_to_postgres` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9c397-691e-4dbe-8f83-c6d225c3b46b",
   "metadata": {},
   "source": [
    "### Updating the flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b9c1b-49c2-45bd-bd94-d634bb4d79bf",
   "metadata": {},
   "source": [
    "Now in the `spotify_workflow.py` file, add the following:\n",
    "    \n",
    "1. New `load_files_to_postgres` task\n",
    "    * This should call the function in the adapter\n",
    "    \n",
    "2. Add the task to the flow\n",
    "    * Add this `load_files_to_postgres` task to the flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee48f26-f7a4-4603-9f14-fc35e87cfb0a",
   "metadata": {},
   "source": [
    "Run the `spotify_workflow.py` file, and confirm that the data is loaded to postgres (there will be a new record with `sample-data` as the track each time you run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab379409-f5fc-4a01-bd42-9ec7ba25ee78",
   "metadata": {},
   "source": [
    "### Setting up Deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f12e3-3ddc-4598-b74f-eebceafa5390",
   "metadata": {},
   "source": [
    "Ok, so now we can see the entrypoint to our prefect workflow as the `extract_and_write` function in the `spotify_workflow.py` file.\n",
    "\n",
    "* `extract_and_write(playlist_id)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d0441e-9dc3-4547-a0f5-7ba62e76b0bc",
   "metadata": {},
   "source": [
    "Next let's turn this into a deployment.  Use the `serve` method to:\n",
    "\n",
    "* name the deployment `get-songs-deployment`\n",
    "* pass through the necessary parameters to call the workflow\n",
    "\n",
    "Then re-run the `spotify_workflow.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8855b8-6846-4bdf-bc71-05fad17bd5ac",
   "metadata": {},
   "source": [
    "* You can see that this created a deployment.\n",
    "\n",
    "From a new tab, run the following:\n",
    "    \n",
    "`prefect server start`\n",
    "\n",
    "And then click on deployments, where you should see the `get-songs-deployment`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511cf5a-3a7f-4210-8c44-bc441b025642",
   "metadata": {},
   "source": [
    "<img src=\"./get-songs-deployment.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a06dbb-c904-4c7c-9a6d-117008d8b554",
   "metadata": {},
   "source": [
    "From there, if you click on the `get-songs-deployment`, followed by the `Runs` or `Upcoming` tabs, you can see that nothing has been run, and nothing is scheduled.  \n",
    "\n",
    "Let's change that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e38e1-143a-4af2-a541-9c5213f3b48e",
   "metadata": {},
   "source": [
    "> First import both the interval schedule and the Deployment class with the following code: \n",
    "\n",
    "```python\n",
    "from prefect.server.schemas.schedules import IntervalSchedule\n",
    "from prefect.deployments.deployments import Deployment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cebbc3-8b20-4dfe-952e-24d3acadec51",
   "metadata": {},
   "source": [
    "Create a prefect deployment using the `Deployment.build_from_flow` method.\n",
    "\n",
    "In doing that you'll need to set the following parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fde2ba-8f48-474f-a3da-97a2f5100452",
   "metadata": {},
   "source": [
    "```python\n",
    "name=\"spotify_deployment\",\n",
    "flow=extract_and_write,\n",
    "version=1,\n",
    "schedule=schedule,\n",
    "is_schedule_active=True,\n",
    "work_queue_name=\"default\",\n",
    "parameters=parameters,\n",
    "entrypoint=\"./spotify_workflow.py:extract_and_write\",\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e86ba0-4524-4887-a67f-9a7975e49b6c",
   "metadata": {},
   "source": [
    "We'll let you assign the parameters to the appropriate Python variable.  And for the schedule, assign this to an instance of the IntervalSchedule, to be run every ten seconds.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa18a7-6f57-4ecd-9746-ec43efc8317b",
   "metadata": {},
   "source": [
    "Make sure to add the Python code to apply the deployment.\n",
    "\n",
    "```python\n",
    "deployment.apply(upload=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3e496-b14e-401a-a839-d3e64dd917f6",
   "metadata": {},
   "source": [
    "And then run the Python script that has this deployment to apply the deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98911a66-270b-4335-b9d8-65f13146622a",
   "metadata": {},
   "source": [
    "From there, you'll need to have to start up the `default-agent-pool`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8b129-e973-48e5-b1d1-d8bc402f393a",
   "metadata": {},
   "source": [
    "```bash\n",
    "prefect agent start -p 'default-agent-pool'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ee82b-0200-41c4-86ad-feb87c93c00b",
   "metadata": {},
   "source": [
    "Now if you navigate to the webserver, and click on deployments you should see your new `spotify_deployment`.  And from there click on the deployment followed by `runs`, and you should see some successful runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7095ecc-4d9e-46cb-a42f-dcfd6f63066f",
   "metadata": {},
   "source": [
    "<img src=\"./success.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47ed95-1a93-43fc-a9f4-dd0994e19851",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resources\n",
    "\n",
    "[Jeff Hale - Deployments](https://medium.com/the-prefect-blog/deploy-prefect-pipelines-with-python-perfect-68c944a3a89f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc250cab-da27-4576-8856-d9c5a84ae9d0",
   "metadata": {},
   "source": [
    "[Kevin Kho](https://medium.com/the-prefect-blog/the-simple-guide-to-productionizing-data-workflows-with-docker-31a5aae67c0a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a4e9f-246b-44d1-907f-bc4c527594e0",
   "metadata": {},
   "source": [
    "[Creating a Deployment](https://discourse.prefect.io/t/error-when-creating-a-deployment-with-the-cli-modulenotfound/2426/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd874f9a-9f77-47db-8d65-cf36d0af851c",
   "metadata": {},
   "source": [
    "[Sample deployment](https://github.com/PrefectHQ/prefect/issues/8710)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
